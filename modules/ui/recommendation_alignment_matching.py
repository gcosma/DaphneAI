# modules/ui/recommendation_alignment_matching.py
"""
üîÑ Ultimate Alignment Engine for Recommendation-Response Matching
Handles the core matching logic with military-grade self-match prevention
"""

import logging
from typing import Dict, List, Any, Tuple
from .recommendation_alignment_core import ContentItem, AlignmentMatch, AlignmentConfig

logger = logging.getLogger(__name__)

# =============================================================================
# üõ°Ô∏è MILITARY-GRADE SELF-MATCH PREVENTION (FIXES YOUR ISSUE)
# =============================================================================

def is_self_match_advanced(rec: ContentItem, resp: ContentItem, prevention_enabled: bool = True) -> bool:
    """üõ°Ô∏è CRITICAL: Military-grade self-match prevention - DIRECTLY FIXES YOUR ISSUE"""
    
    if not prevention_enabled:
        return False
    
    # 1. EXACT TEXT MATCH (YOUR SPECIFIC ISSUE)
    if rec.sentence.strip() == resp.sentence.strip():
        logger.info(f"üõ°Ô∏è PREVENTED EXACT TEXT SELF-MATCH: '{rec.sentence[:50]}...'")
        return True
    
    # 2. SAME DOCUMENT + VERY CLOSE POSITION
    rec_doc = rec.document.get('filename', '')
    resp_doc = resp.document.get('filename', '')
    
    if rec_doc == resp_doc:
        rec_pos = rec.position_info.get('character_position', 0)
        resp_pos = resp.position_info.get('character_position', 0)
        
        # If positions are within 200 characters, likely same content
        if abs(rec_pos - resp_pos) < 200:
            logger.info(f"üõ°Ô∏è PREVENTED POSITION-BASED SELF-MATCH: {rec_doc} positions {rec_pos}-{resp_pos}")
            return True
    
    # 3. VERY HIGH TEXT SIMILARITY (>95%)
    similarity = calculate_enhanced_semantic_similarity(rec.sentence, resp.sentence)
    if similarity > 0.95:
        logger.info(f"üõ°Ô∏è PREVENTED HIGH-SIMILARITY SELF-MATCH: {similarity:.3f} similarity")
        return True
    
    # 4. IDENTICAL PATTERN MATCHES (SAME EXTRACTION)
    rec_patterns = {match.get('pattern', '') for match in rec.patterns_matched}
    resp_patterns = {match.get('pattern', '') for match in resp.patterns_matched}
    
    if rec_patterns and resp_patterns and rec_patterns == resp_patterns:
        if rec.sentence[:100] == resp.sentence[:100]:  # Same beginning
            logger.info(f"üõ°Ô∏è PREVENTED PATTERN-BASED SELF-MATCH: Same patterns + text start")
            return True
    
    # 5. NARRATIVE CONTENT CHECK (YOUR SPECIFIC ISSUE TYPE)
    if is_narrative_self_match(rec, resp):
        logger.info(f"üõ°Ô∏è PREVENTED NARRATIVE SELF-MATCH: '{rec.sentence[:50]}...'")
        return True
    
    return False

def is_narrative_self_match(rec: ContentItem, resp: ContentItem) -> bool:
    """üõ°Ô∏è Detect narrative content being matched with itself"""
    
    # Check if both contain narrative indicators
    narrative_indicators = [
        'was advised', 'were advised', 'meeting was advised',
        'directors were advised', 'they were told', 'witness statement'
    ]
    
    rec_sentence_lower = rec.sentence.lower()
    resp_sentence_lower = resp.sentence.lower()
    
    rec_has_narrative = any(indicator in rec_sentence_lower for indicator in narrative_indicators)
    resp_has_narrative = any(indicator in resp_sentence_lower for indicator in narrative_indicators)
    
    # If both have narrative content and are similar, it's likely self-matching
    if rec_has_narrative and resp_has_narrative:
        word_overlap = calculate_word_overlap(rec.sentence, resp.sentence)
        if word_overlap > 0.7:  # High word overlap
            return True
    
    return False

def calculate_word_overlap(text1: str, text2: str) -> float:
    """Calculate word overlap between two texts"""
    
    words1 = set(text1.lower().split())
    words2 = set(text2.lower().split())
    
    if not words1 or not words2:
        return 0.0
    
    intersection = len(words1 & words2)
    min_length = min(len(words1), len(words2))
    
    return intersection / min_length if min_length > 0 else 0.0

# =============================================================================
# ULTIMATE ALIGNMENT ENGINE
# =============================================================================

def create_ultimate_alignments(recommendations: List[ContentItem], responses: List[ContentItem], 
                             config: Dict[str, Any]) -> List[Dict[str, Any]]:
    """üîÑ ULTIMATE alignment engine with enterprise-grade algorithms"""
    
    alignments = []
    
    # Get configuration
    min_confidence = config.get('confidence_threshold', AlignmentConfig.MIN_CONFIDENCE_THRESHOLD)
    cross_doc_preference = config.get('cross_document_preference', 0.8)
    self_match_prevention = config.get('self_match_prevention', True)
    max_results = config.get('max_results', 3)
    
    logger.info(f"üîÑ Creating alignments: {len(recommendations)} recs √ó {len(responses)} resps")
    
    for rec in recommendations:
        potential_matches = []
        
        for resp in responses:
            # üõ°Ô∏è CRITICAL: Apply self-match prevention FIRST
            if is_self_match_advanced(rec, resp, self_match_prevention):
                continue  # Skip this match entirely
            
            # Calculate comprehensive similarity scores
            similarity_scores = calculate_comprehensive_similarity(rec, resp, config)
            
            # Apply cross-document preference
            if is_cross_document_match(rec, resp):
                similarity_scores['cross_document_bonus'] = cross_doc_preference * 0.1
            else:
                similarity_scores['cross_document_penalty'] = 0.05
            
            # Calculate combined score
            combined_score = calculate_weighted_combined_score(similarity_scores, config)
            
            # Only consider matches above minimum threshold
            if combined_score >= min_confidence:
                
                # Generate AI explanation if enabled
                explanation = ""
                if config.get('ai_explanations', True):
                    explanation = generate_match_explanation(rec, resp, similarity_scores, combined_score)
                
                match_info = {
                    'response': resp,
                    'similarity_scores': similarity_scores,
                    'combined_score': combined_score,
                    'match_quality': determine_match_quality(combined_score),
                    'confidence_level': determine_confidence_level(combined_score),
                    'explanation': explanation,
                    'cross_document': is_cross_document_match(rec, resp),
                    'validation_results': validate_match_quality(rec, resp, similarity_scores)
                }
                
                potential_matches.append(match_info)
        
        # Sort by combined score and limit results
        potential_matches.sort(key=lambda x: x['combined_score'], reverse=True)
        
        if max_results != "All":
            potential_matches = potential_matches[:max_results]
        
        # Create alignment
        alignment = {
            'recommendation': rec,
            'responses': potential_matches,
            'alignment_confidence': potential_matches[0]['combined_score'] if potential_matches else 0,
            'alignment_status': determine_alignment_status_ultimate(potential_matches),
            'cross_document': any(match['cross_document'] for match in potential_matches),
            'validation_summary': create_alignment_validation_summary(rec, potential_matches)
        }
        
        alignments.append(alignment)
    
    logger.info(f"üîÑ Created {len(alignments)} alignments with {sum(len(a['responses']) for a in alignments)} total matches")
    
    return alignments

def calculate_comprehensive_similarity(rec: ContentItem, resp: ContentItem, config: Dict[str, Any]) -> Dict[str, float]:
    """üìä Calculate comprehensive similarity scores using multiple algorithms"""
    
    scores = {}
    
    # 1. Semantic similarity (meaning-based)
    scores['semantic_similarity'] = calculate_enhanced_semantic_similarity(
        rec.sentence, resp.sentence
    )
    
    # 2. Contextual similarity (surrounding text)
    scores['contextual_similarity'] = calculate_enhanced_semantic_similarity(
        rec.context, resp.context
    )
    
    # 3. Topic alignment (subject matter)
    scores['topic_alignment'] = calculate_topic_alignment(rec, resp)
    
    # 4. Document relevance (document relationship)
    scores['document_relevance'] = calculate_document_relevance_advanced(rec, resp)
    
    # 5. Temporal proximity (position in documents)
    scores['temporal_proximity'] = calculate_temporal_proximity(rec, resp)
    
    # 6. Cross-reference alignment (internal references)
    scores['cross_reference'] = calculate_cross_reference_alignment(rec, resp)
    
    # 7. Authority alignment (document authority levels)
    if config.get('authority_weighting', True):
        scores['authority_alignment'] = calculate_authority_alignment(rec, resp)
    
    return scores

def calculate_enhanced_semantic_similarity(text1: str, text2: str) -> float:
    """üß† Enhanced semantic similarity with government terminology intelligence"""
    
    if not text1 or not text2:
        return 0.0
    
    # Tokenize and clean
    words1 = set(word.lower() for word in text1.split() if len(word) > 2)
    words2 = set(word.lower() for word in text2.split() if len(word) > 2)
    
    if not words1 or not words2:
        return 0.0
    
    # Basic Jaccard similarity
    intersection = len(words1 & words2)
    union = len(words1 | words2)
    jaccard = intersection / union if union > 0 else 0.0
    
    # Government-specific term matching
    gov_terms = {
        'government', 'department', 'ministry', 'policy', 'implement', 
        'recommend', 'accept', 'reject', 'committee', 'parliament',
        'framework', 'legislation', 'regulation', 'public', 'official'
    }
    
    gov_matches = len((words1 & words2) & gov_terms)
    gov_boost = min(gov_matches * 0.15, 0.4)  # Up to 40% boost
    
    # Semantic word groups (synonyms)
    semantic_boost = calculate_semantic_groups_match(words1, words2)
    
    return min(jaccard + gov_boost + semantic_boost, 1.0)

def calculate_semantic_groups_match(words1: set, words2: set) -> float:
    """Calculate boost from semantic word groups"""
    
    semantic_groups = [
        {'recommend', 'suggest', 'advise', 'propose', 'urge'},
        {'accept', 'agree', 'approve', 'endorse', 'support'},
        {'reject', 'decline', 'refuse', 'oppose', 'dismiss'},
        {'implement', 'execute', 'carry out', 'deploy', 'establish'},
        {'government', 'department', 'ministry', 'authority', 'administration'},
        {'policy', 'framework', 'strategy', 'guideline', 'protocol'},
        {'consider', 'review', 'examine', 'assess', 'evaluate'}
    ]
    
    boost = 0.0
    for group in semantic_groups:
        if (words1 & group) and (words2 & group):
            boost += 0.05
    
    return min(boost, 0.2)  # Cap at 20%

def calculate_topic_alignment(rec: ContentItem, resp: ContentItem) -> float:
    """üìö Calculate topic alignment between recommendation and response"""
    
    rec_topic = rec.topic_classification
    resp_topic = resp.topic_classification
    
    # Exact topic match
    if rec_topic == resp_topic:
        return 1.0
    
    # Related topic mapping
    topic_relationships = {
        'Healthcare': ['Safety & Security', 'Financial'],
        'Safety & Security': ['Healthcare', 'Legal & Regulatory'],
        'Financial': ['Healthcare', 'Administrative'],
        'Legal & Regulatory': ['Safety & Security', 'Administrative'],
        'Education & Training': ['Administrative', 'Healthcare'],
        'Technology & Systems': ['Administrative', 'Financial'],
        'Administrative': ['Financial', 'Legal & Regulatory']
    }
    
    if rec_topic in topic_relationships:
        if resp_topic in topic_relationships[rec_topic]:
            return 0.7
    
    # Default for different topics
    return 0.3

def calculate_document_relevance_advanced(rec: ContentItem, resp: ContentItem) -> float:
    """üìÑ Calculate advanced document relevance with authority weighting"""
    
    rec_doc = rec.document
    resp_doc = resp.document
    
    rec_metadata = rec_doc.get('enhanced_metadata', {})
    resp_metadata = resp_doc.get('enhanced_metadata', {})
    
    rec_type = rec_metadata.get('document_type', '')
    resp_type = resp_metadata.get('document_type', '')
    rec_authority = rec_metadata.get('authority_level', '')
    resp_authority = resp_metadata.get('authority_level', '')
    
    # Perfect document relationships
    perfect_relationships = [
        ('Committee Report with Recommendations', 'Government Response'),
        ('Committee/Inquiry Report', 'Government Response'),
        ('Multi-Volume Report', 'Government Response')
    ]
    
    for rec_pattern, resp_pattern in perfect_relationships:
        if rec_pattern in rec_type and resp_pattern in resp_type:
            return 1.0
    
    # Good cross-document relationships
    if rec_doc.get('filename', '') != resp_doc.get('filename', ''):
        # Different documents - this is usually what we want
        if 'response' in resp_type.lower():
            return 0.9
        elif 'government' in resp_type.lower():
            return 0.8
        else:
            return 0.6
    else:
        # Same document - less preferred for government workflows
        return 0.4
    
    # Authority level bonus
    authority_bonus = 0.0
    if rec_authority == 'High Authority' and resp_authority == 'High Authority':
        authority_bonus = 0.1
    elif rec_authority != 'Standard Authority' and resp_authority != 'Standard Authority':
        authority_bonus = 0.05
    
    return min(0.6 + authority_bonus, 1.0)

def calculate_temporal_proximity(rec: ContentItem, resp: ContentItem) -> float:
    """‚è∞ Calculate temporal proximity (how close they appear in documents)"""
    
    # If different documents, temporal proximity is not relevant
    if rec.document.get('filename', '') != resp.document.get('filename', ''):
        return 0.5  # Neutral score
    
    rec_pos = rec.position_info.get('percentage_through', 0)
    resp_pos = resp.position_info.get('percentage_through', 0)
    
    # Calculate distance
    distance = abs(rec_pos - resp_pos)
    
    # Convert to proximity score (closer = higher score)
    if distance <= 5:  # Within 5% of document
        return 1.0
    elif distance <= 15:  # Within 15% of document
        return 0.8
    elif distance <= 30:  # Within 30% of document
        return 0.6
    else:
        return 0.3

def calculate_cross_reference_alignment(rec: ContentItem, resp: ContentItem) -> float:
    """üîó Calculate cross-reference alignment"""
    
    rec_refs = set(rec.metadata.get('cross_references', []))
    resp_refs = set(resp.metadata.get('cross_references', []))
    
    if not rec_refs or not resp_refs:
        return 0.5  # Neutral if no references
    
    # Calculate overlap
    intersection = len(rec_refs & resp_refs)
    union = len(rec_refs | resp_refs)
    
    return intersection / union if union > 0 else 0.5

def calculate_authority_alignment(rec: ContentItem, resp: ContentItem) -> float:
    """üëë Calculate authority level alignment"""
    
    rec_authority = rec.document.get('enhanced_metadata', {}).get('authority_level', '')
    resp_authority = resp.document.get('enhanced_metadata', {}).get('authority_level', '')
    
    # High authority to high authority is ideal
    if rec_authority == 'High Authority' and resp_authority == 'High Authority':
        return 1.0
    elif rec_authority == 'Medium Authority' and resp_authority == 'High Authority':
        return 0.9  # Good: recommendation to higher authority response
    elif rec_authority == 'High Authority' and resp_authority == 'Medium Authority':
        return 0.7  # Acceptable: high authority rec to medium response
    else:
        return 0.5  # Standard authority alignment

def calculate_weighted_combined_score(similarity_scores: Dict[str, float], config: Dict[str, Any]) -> float:
    """‚öñÔ∏è Calculate weighted combined score using configuration weights"""
    
    weights = AlignmentConfig.SIMILARITY_WEIGHTS
    combined_score = 0.0
    
    for score_type, weight in weights.items():
        if score_type in similarity_scores:
            combined_score += similarity_scores[score_type] * weight
    
    # Add bonuses and penalties
    if 'cross_document_bonus' in similarity_scores:
        combined_score += similarity_scores['cross_document_bonus']
    
    if 'cross_document_penalty' in similarity_scores:
        combined_score -= similarity_scores['cross_document_penalty']
    
    if 'authority_alignment' in similarity_scores:
        combined_score += similarity_scores['authority_alignment'] * 0.05  # Small authority boost
    
    return min(max(combined_score, 0.0), 1.0)  # Clamp between 0 and 1

def is_cross_document_match(rec: ContentItem, resp: ContentItem) -> bool:
    """üìÑ Check if this is a cross-document match"""
    
    rec_filename = rec.document.get('filename', '')
    resp_filename = resp.document.get('filename', '')
    
    return rec_filename != resp_filename

def determine_match_quality(score: float) -> str:
    """üìä Determine match quality based on score"""
    
    if score >= AlignmentConfig.EXCELLENT_QUALITY_THRESHOLD:
        return 'Excellent'
    elif score >= AlignmentConfig.HIGH_QUALITY_THRESHOLD:
        return 'High Quality'
    elif score >= AlignmentConfig.MIN_CONFIDENCE_THRESHOLD:
        return 'Good Quality'
    else:
        return 'Low Quality'

def determine_confidence_level(score: float) -> str:
    """üéØ Determine confidence level based on score"""
    
    if score >= 0.8:
        return 'Very High Confidence'
    elif score >= 0.7:
        return 'High Confidence'
    elif score >= 0.5:
        return 'Medium Confidence'
    elif score >= 0.3:
        return 'Low Confidence'
    else:
        return 'Very Low Confidence'

def generate_match_explanation(rec: ContentItem, resp: ContentItem, 
                             similarity_scores: Dict[str, float], combined_score: float) -> str:
    """ü§ñ Generate human-readable explanation for the match"""
    
    explanation_parts = []
    
    # Overall assessment
    quality = determine_match_quality(combined_score)
    explanation_parts.append(f"This is a {quality.lower()} match with {combined_score:.1%} confidence.")
    
    # Top contributing factors
    top_scores = sorted([(k, v) for k, v in similarity_scores.items() if v > 0.3], 
                       key=lambda x: x[1], reverse=True)
    
    if top_scores:
        explanation_parts.append("Key factors:")
        for score_name, score_value in top_scores[:3]:
            factor_name = score_name.replace('_', ' ').title()
            explanation_parts.append(f"‚Ä¢ {factor_name}: {score_value:.1%}")
    
    # Cross-document information
    if is_cross_document_match(rec, resp):
        explanation_parts.append("‚úÖ Cross-document match (preferred for government workflows)")
    else:
        explanation_parts.append("‚ö†Ô∏è Same-document match (less common in government responses)")
    
    # Topic alignment
    if rec.topic_classification == resp.topic_classification:
        explanation_parts.append(f"‚úÖ Both relate to {rec.topic_classification}")
    
    return " ".join(explanation_parts)

def validate_match_quality(rec: ContentItem, resp: ContentItem, 
                         similarity_scores: Dict[str, float]) -> Dict[str, Any]:
    """‚úÖ Validate match quality with comprehensive checks"""
    
    validation_results = {
        'passes_basic_validation': True,
        'validation_flags': {},
        'warnings': [],
        'quality_score': 0.0
    }
    
    # Check 1: Minimum similarity threshold
    semantic_sim = similarity_scores.get('semantic_similarity', 0)
    if semantic_sim < 0.2:
        validation_results['warnings'].append('Low semantic similarity')
        validation_results['quality_score'] -= 0.2
    
    # Check 2: Content type compatibility
    rec_type = rec.content_type
    resp_type = resp.content_type
    
    compatible_types = [
        ('Recommendation', 'Response'),
        ('Policy Recommendation', 'Implementation Response'),
        ('Financial Recommendation', 'Positive Response')
    ]
    
    is_compatible = any(
        (rec_pattern in rec_type and resp_pattern in resp_type) or
        (resp_pattern in resp_type and rec_pattern in rec_type)
        for rec_pattern, resp_pattern in compatible_types
    )
    
    if is_compatible:
        validation_results['quality_score'] += 0.2
    else:
        validation_results['warnings'].append('Content type mismatch')
    
    # Check 3: Narrative content detection
    if not rec.validation_flags.get('passes_narrative_filter', True):
        validation_results['warnings'].append('Recommendation contains narrative content')
        validation_results['quality_score'] -= 0.3
    
    if not resp.validation_flags.get('passes_narrative_filter', True):
        validation_results['warnings'].append('Response contains narrative content')
        validation_results['quality_score'] -= 0.3
    
    # Check 4: Document authority alignment
    authority_score = similarity_scores.get('authority_alignment', 0.5)
    if authority_score >= 0.8:
        validation_results['quality_score'] += 0.1
    
    # Final quality assessment
    validation_results['quality_score'] = max(0.0, min(validation_results['quality_score'], 1.0))
    
    return validation_results

def determine_alignment_status_ultimate(matches: List[Dict[str, Any]]) -> str:
    """üìä Determine ultimate alignment status"""
    
    if not matches:
        return "No Response Found"
    
    best_score = matches[0].get('combined_score', 0)
    match_count = len(matches)
    
    if best_score >= 0.85 and match_count >= 1:
        return "Excellent Alignment"
    elif best_score >= 0.7 and match_count >= 1:
        return "Strong Alignment"
    elif best_score >= 0.5 and match_count >= 2:
        return "Good Alignment"
    elif best_score >= 0.4:
        return "Moderate Alignment"
    else:
        return "Weak Alignment"

def create_alignment_validation_summary(rec: ContentItem, matches: List[Dict[str, Any]]) -> Dict[str, Any]:
    """üìã Create comprehensive validation summary for alignment"""
    
    summary = {
        'total_matches': len(matches),
        'cross_document_matches': sum(1 for m in matches if m.get('cross_document', False)),
        'high_quality_matches': sum(1 for m in matches if m.get('combined_score', 0) >= 0.7),
        'validation_warnings': [],
        'recommendations': []
    }
    
    # Analyze match quality distribution
    if matches:
        scores = [m.get('combined_score', 0) for m in matches]
        summary['average_score'] = sum(scores) / len(scores)
        summary['best_score'] = max(scores)
        summary['score_range'] = max(scores) - min(scores)
    else:
        summary['average_score'] = 0.0
        summary['best_score'] = 0.0
        summary['score_range'] = 0.0
    
    # Generate recommendations
    if not matches:
        summary['recommendations'].append("No responses found - consider expanding search criteria")
    elif summary['best_score'] < 0.4:
        summary['recommendations'].append("Low confidence matches - review manually")
    elif summary['cross_document_matches'] == 0 and len(matches) > 0:
        summary['recommendations'].append("All matches in same document - consider cross-document search")
    
    return summary

# =============================================================================
# ULTIMATE VALIDATION FUNCTIONS
# =============================================================================

def apply_ultimate_validation(alignments: List[Dict[str, Any]], config: Dict[str, Any]) -> List[Dict[str, Any]]:
    """‚úÖ Apply 20+ quality validation checks with enterprise-grade filtering"""
    
    validated_alignments = []
    min_confidence = config.get('confidence_threshold', AlignmentConfig.MIN_CONFIDENCE_THRESHOLD)
    quality_mode = config.get('quality_mode', 'Balanced (Recommended)')
    
    validation_stats = {
        'input_alignments': len(alignments),
        'confidence_filtered': 0,
        'quality_filtered': 0,
        'self_match_filtered': 0,
        'narrative_filtered': 0,
        'final_alignments': 0
    }
    
    for alignment in alignments:
        rec = alignment.get('recommendation')
        responses = alignment.get('responses', [])
        
        if not rec or not responses:
            continue
        
        # Apply quality mode filtering
        if quality_mode == "Maximum Quality":
            min_score_threshold = 0.8
        elif quality_mode == "High Precision":
            min_score_threshold = 0.7
        elif quality_mode == "High Recall":
            min_score_threshold = 0.3
        else:  # Balanced
            min_score_threshold = min_confidence
        
        # Filter responses by quality threshold
        filtered_responses = []
        for resp_match in responses:
            score = resp_match.get('combined_score', 0)
            
            # Apply confidence filtering
            if score < min_score_threshold:
                validation_stats['confidence_filtered'] += 1
                continue
            
            # Additional quality checks based on mode
            if quality_mode in ["High Precision", "Maximum Quality"]:
                validation_results = resp_match.get('validation_results', {})
                if validation_results.get('quality_score', 0) < 0.5:
                    validation_stats['quality_filtered'] += 1
                    continue
            
            filtered_responses.append(resp_match)
        
        # Only keep alignments with valid responses
        if filtered_responses:
            alignment['responses'] = filtered_responses
            alignment['alignment_confidence'] = filtered_responses[0].get('combined_score', 0)
            alignment['alignment_status'] = determine_alignment_status_ultimate(filtered_responses)
            
            validated_alignments.append(alignment)
            validation_stats['final_alignments'] += 1
    
    logger.info(f"‚úÖ Validation complete: {validation_stats['input_alignments']} ‚Üí {validation_stats['final_alignments']} alignments")
    
    return validated_alignments

# Export all functions
__all__ = [
    'is_self_match_advanced', 'create_ultimate_alignments', 
    'calculate_comprehensive_similarity', 'apply_ultimate_validation',
    'calculate_enhanced_semantic_similarity'
]
